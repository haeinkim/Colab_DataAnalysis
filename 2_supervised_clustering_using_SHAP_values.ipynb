{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21546,"status":"ok","timestamp":1686334923426,"user":{"displayName":"Haein Kim","userId":"06234233614363415911"},"user_tz":240},"id":"OF7vBaHSu-JX","outputId":"d34ec2cb-d8db-4f35-dbbf-56fdab74012e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting shap\n","  Downloading shap-0.41.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (572 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.6/572.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.10.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n","Requirement already satisfied: tqdm\u003e4.25.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.65.0)\n","Requirement already satisfied: packaging\u003e20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.1)\n","Collecting slicer==0.0.7 (from shap)\n","  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.56.4)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n","Requirement already satisfied: llvmlite\u003c0.40,\u003e=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba-\u003eshap) (0.39.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba-\u003eshap) (67.7.2)\n","Requirement already satisfied: python-dateutil\u003e=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003eshap) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003eshap) (2022.7.1)\n","Requirement already satisfied: joblib\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-\u003eshap) (1.2.0)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-\u003eshap) (3.1.0)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil\u003e=2.8.1-\u003epandas-\u003eshap) (1.16.0)\n","Installing collected packages: slicer, shap\n","Successfully installed shap-0.41.0 slicer-0.0.7\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting umap-learn\n","  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.22.4)\n","Requirement already satisfied: scikit-learn\u003e=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n","Requirement already satisfied: scipy\u003e=1.0 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.10.1)\n","Requirement already satisfied: numba\u003e=0.49 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.56.4)\n","Collecting pynndescent\u003e=0.5 (from umap-learn)\n","  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.65.0)\n","Requirement already satisfied: llvmlite\u003c0.40,\u003e=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba\u003e=0.49-\u003eumap-learn) (0.39.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba\u003e=0.49-\u003eumap-learn) (67.7.2)\n","Requirement already satisfied: joblib\u003e=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent\u003e=0.5-\u003eumap-learn) (1.2.0)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn\u003e=0.22-\u003eumap-learn) (3.1.0)\n","Building wheels for collected packages: umap-learn, pynndescent\n","  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82816 sha256=dba3fb5e10846961918e59ede0c931c594efcefa807cf014e037f201a611d357\n","  Stored in directory: /root/.cache/pip/wheels/a0/e8/c6/a37ea663620bd5200ea1ba0907ab3c217042c1d035ef606acc\n","  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55622 sha256=3f23a73c3b5fb33f72606e586430308baf2d8348bd4f8786fcbe3c6f988fd48a\n","  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n","Successfully built umap-learn pynndescent\n","Installing collected packages: pynndescent, umap-learn\n","Successfully installed pynndescent-0.5.10 umap-learn-0.5.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting optuna\n","  Downloading optuna-3.2.0-py3-none-any.whl (390 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.6/390.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting alembic\u003e=1.5.0 (from optuna)\n","  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cmaes\u003e=0.9.1 (from optuna)\n","  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n","Requirement already satisfied: sqlalchemy\u003e=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.10)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0)\n","Collecting Mako (from alembic\u003e=1.5.0-\u003eoptuna)\n","  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions\u003e=4 in /usr/local/lib/python3.10/dist-packages (from alembic\u003e=1.5.0-\u003eoptuna) (4.5.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy\u003e=1.3.0-\u003eoptuna) (2.0.2)\n","Requirement already satisfied: MarkupSafe\u003e=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako-\u003ealembic\u003e=1.5.0-\u003eoptuna) (2.1.2)\n","Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n","Successfully installed Mako-1.2.4 alembic-1.11.1 cmaes-0.9.1 colorlog-6.7.0 optuna-3.2.0\n"]}],"source":["!pip install shap\n","!pip install umap-learn\n","!pip install optuna"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":11942,"status":"ok","timestamp":1686340951176,"user":{"displayName":"Haein Kim","userId":"06234233614363415911"},"user_tz":240},"id":"J3Mib7MpubpW"},"outputs":[],"source":["import os\n","import pickle as pickle\n","import glob\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","import lightgbm as lgbm\n","import shap\n","\n","from scipy.spatial.transform import Rotation\n","import time\n","from joblib import Parallel, delayed\n","import umap\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1513,"status":"ok","timestamp":1686340952675,"user":{"displayName":"Haein Kim","userId":"06234233614363415911"},"user_tz":240},"id":"vlSzc1h9uChl","outputId":"aee1fe07-3e75-4460-9818-b9285512c39c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1686340952675,"user":{"displayName":"Haein Kim","userId":"06234233614363415911"},"user_tz":240},"id":"3GsW9k3IAVCr"},"outputs":[],"source":["import optuna  # pip install optuna\n","from sklearn.metrics import log_loss\n","from sklearn.model_selection import StratifiedKFold\n","from optuna.integration import LightGBMPruningCallback\n","\n","def objective(trial, X, y):\n","    param_grid = {\n","        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n","        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n","        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n","        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n","        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n","        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n","        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n","        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n","        \"bagging_fraction\": trial.suggest_float(\n","            \"bagging_fraction\", 0.2, 0.9, step=0.1\n","        ),\n","        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n","        \"feature_fraction\": trial.suggest_float(\n","            \"feature_fraction\", 0.2, 0.9, step=0.1\n","        ),\n","    }\n","\n","    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1121218)\n","\n","    cv_scores = np.empty(5)\n","    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n","        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n","        \n","        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n","        model = lgbm.LGBMClassifier(objective=\"binary\", **param_grid)\n","        model.fit(\n","            X_train,\n","            y_train,\n","            eval_set=[(X_test, y_test)],\n","            eval_metric=\"binary_logloss\",\n","            early_stopping_rounds=100,\n","            callbacks=[\n","                LightGBMPruningCallback(trial, \"binary_logloss\")\n","            ],  # Add a pruning callback\n","        )\n","        preds = model.predict_proba(X_test)\n","        cv_scores[idx] = log_loss(y_test, preds)\n","\n","    return np.mean(cv_scores)\n","\n","# study = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Classifier\")\n","# func = lambda trial: objective(trial, X, y)\n","# study.optimize(func, n_trials=20)\n","\n","# print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n","# print(f\"\\tBest params:\")\n","\n","# for key, value in study.best_params.items():\n","#     print(f\"\\t\\t{key}: {value}\")\n","    \n","# -----------------------------------------------------\n","# Best value (binary_logloss): 0.35738\n","# \tBest params:\n","# \t\tdevice: gpu\n","# \t\tlambda_l1: 7.71800699380605e-05\n","# \t\tlambda_l2: 4.17890272377219e-06\n","# \t\tbagging_fraction: 0.7000000000000001\n","# \t\tfeature_fraction: 0.4\n","# \t\tbagging_freq: 5\n","# \t\tmax_depth: 5\n","# \t\tnum_leaves: 1007\n","# \t\tmin_data_in_leaf: 45\n","# \t\tmin_split_gain: 15.703519227860273\n","# \t\tlearning_rate: 0.010784015325759629\n","# \t\tn_estimators: 10000"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1uMvfVRGjrUD_-99oosInfxdlKRGYJMHx"},"id":"sl5f-9-7vNTj","outputId":"a3a69cd7-765c-4d33-b163-aeba5f9f3f47"},"outputs":[],"source":["# Define the directory where the pickle files are located\n","pickle_directory = '/content/drive/My Drive/00_project/05_fly-arena/10_Locomotion/output_windows/all/'\n","\n","dfs = []  # List to store individual dataframes\n","\n","for filename in os.listdir(pickle_directory):\n","    print(f\"Processing file: {filename}\")\n","    if filename.endswith('.pkl'):\n","        # Extract the window size from the filename\n","        window_size = int(filename.split('_')[1])\n","\n","        if window_size == 10:\n","\n","            # Read the pickle file\n","            with open(os.path.join(pickle_directory, filename), 'rb') as f:\n","                data = pickle.load(f)\n","                \n","            # Select columns based on window_size\n","            columns = data.columns[:window_size * 4]\n","            # print(data.head())\n","            print(f\"DataFrame selected... column size: {window_size * 4}\")\n","            y = data.iloc[:, -1]\n","            X = data[columns]\n","\n","            print(f\"Running LGBM optimization using Optuna... column size: {window_size * 4}\")\n","\n","            study = optuna.create_study(direction=\"minimize\", \n","                                        study_name=\"LGBM Classifier\", \n","                                        sampler=optuna.samplers.RandomSampler())\n","            \n","            func = lambda trial: objective(trial, X, y)\n","            study.optimize(func, n_trials=30)\n","\n","            fig = optuna.visualization.plot_optimization_history(study)\n","            fig.show()\n","\n","            print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n","            print(f\"\\tBest params:\")\n","\n","            for key, value in study.best_params.items():\n","                print(f\"\\t\\t{key}: {value}\")\n","\n","\n","\n","        #     # fit a GBT model to the data\n","        #     m = lgbm.LGBMClassifier()\n","        #     m.fit(X, y)\n","\n","        #     # compute SHAP values\n","        #     explainer = shap.Explainer(m)\n","        #     shap_values = explainer(X)\n","            \n","        # # compute 2D embedding of raw variable values\n","        #     X_2d = umap.UMAP(\n","        #     n_components=2, n_neighbors=200, min_dist=0\n","        #     ).fit_transform(X)\n","\n","        #     # compute 2D embedding of SHAP values\n","        #     s_2d = umap.UMAP(\n","        #     n_components=2, n_neighbors=200, min_dist=0\n","        #     ).fit_transform(shap_values.values[:, :, 1])\n","\n","        #     plt.scatter(\n","        #             X_2d[:, 0],\n","        #             X_2d[:, 1],\n","        #             s=0.2, \n","        #             alpha=0.3,\n","        #             c='r' \n","        #             )\n","        #     plt.title(f'UMAP projection of {window_size}', fontsize=24)\n","\n","        #     plt.scatter(\n","        #             s_2d[:, 0],\n","        #             s_2d[:, 1],\n","        #             s=0.2, \n","        #             alpha=0.3, \n","        #             )\n","        #     plt.title(f'SHAP UMAP projection of {window_size}', fontsize=24)\n","\n","        #     plt.show()    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VfcUu8yP0LAd"},"outputs":[],"source":["# def bootstrap (X, method, B, sigma_noise=None, no_bootstrap=False, random_seed=None, num_jobs=None, use_n_pcs=False, subsample=False, **kwargs):\n","#     '''\n","#     Creates n bootstrap data from X and creates a DR visualizastion for each of them.\n","    \n","#     Arguments:\n","#         See generate() for details\n","    \n","#     Returns:\n","#         X_embedded_list = list of the 2D DR visualization embeddings (numpy arrays)\n","#         bootstrap_indices_list = list of numpy arrays indicating the bootstrap row indices\n","#     '''\n","\n","#     X_embedded_list = []\n","#     bootstrap_indices_list = []\n","    \n","#     # generate sequence of random states if random_seed is specified\n","#     if isinstance(random_seed, int):\n","#         seeded_rand1 = np.random.RandomState(random_seed)\n","#         random_seeded_sequence = seeded_rand1.randint(0,1e6,B)\n","#     else:\n","#         random_seeded_sequence = False\n","    \n","#     # bootstrap DR\n","#     if isinstance(num_jobs, int): # in parallel\n","#         result = Parallel(n_jobs=num_jobs)(delayed(run_one_bootstrap)(X, method, sigma_noise, no_bootstrap,\n","#                                            random_seeded_sequence, b, use_n_pcs, subsample, **kwargs) for b in tqdm(range(B)))\n","#         X_embedded_list = [x[0] for x in result]\n","#         bootstrap_indices_list = [x[1] for x in result]\n","#     else: # using only one core\n","#         for b in tqdm(range(B)):\n","#             X_embedded, boot_idxs = run_one_bootstrap(X, method, sigma_noise, no_bootstrap,\n","#                                            random_seeded_sequence, b, use_n_pcs, subsample, **kwargs)\n","#             X_embedded_list.append(X_embedded)\n","#             bootstrap_indices_list.append(boot_idxs)\n","    \n","#     return(X_embedded_list, bootstrap_indices_list)\n","\n","\n","# def run_one_bootstrap(X, method, sigma_noise=None, no_bootstrap=False, random_seeded_sequence=False, b=0, use_n_pcs=False, subsample=False, **kwargs):\n","#     '''\n","#     Method for generating one bootstrap X and one DR visualization of the bootstrap\n","    \n","#     Arguments:\n","#         random_seeded_sequence = array or list of random seeds to use in generating the bootstrap sample\n","#         b = integer specifying the index of random see in random_seeded_sequence to use for generating the bootstrap\n","#         See generate() for more details\n","    \n","#     Returns:\n","#         X_embedded = 2D DR visualization embedding (numpy array)\n","#         boot_idxs = numpy array indicating the bootstrap row indices\n","#     '''\n","#     # Create bootstrap X\n","#     if subsample is False:\n","#         if no_bootstrap is True: # don't bootstrap (will use intrinsic stochasticity of DR algorithm (if any) only)\n","#             boot_X = X.copy()\n","#             boot_idxs = np.arange(X.shape[0]) # set indices to be the original indices\n","#         elif random_seeded_sequence is not False: # use specified random_seeded_sequence to generate bootstrap X\n","#             seeded_rand2 = np.random.RandomState(random_seeded_sequence[b])\n","#             boot_idxs = seeded_rand2.randint(0,X.shape[0],X.shape[0])\n","#             boot_X = X.copy()[boot_idxs,:] \n","#         else: # if no random_seeded_sequence, use default random process\n","#             boot_idxs = np.random.randint(0,X.shape[0],X.shape[0])\n","#             boot_X = X.copy()[boot_idxs,:]\n","#     # Subsample instead of bootstrapping\n","#     else:\n","#         if random_seeded_sequence is not False: # use specified random_seeded_sequence to generate subsample of X\n","#             seeded_rand2 = np.random.RandomState(random_seeded_sequence[b])\n","#             boot_idxs = seeded_rand2.choice(X.shape[0], subsample, replace=False)\n","#             boot_X = X.copy()[boot_idxs,:] \n","#         else: # if no random_seeded_sequence, use default random process\n","#             boot_idxs = np.random.choice(X.shape[0], subsample, replace=False)\n","#             boot_X = X.copy()[boot_idxs,:]\n","        \n","#     # add Gaussian noise to alleviate duplicate issues if specified\n","#     if sigma_noise is not None:\n","#         boot_X += np.random.normal(0,sigma_noise,boot_X.shape)\n","        \n","#     # run TruncatedSVD if specified to do a first pass DR with PCA and take use_n_pcs top principal components for DR visualization\n","#     if use_n_pcs is not False:\n","#         boot_X = TruncatedSVD(n_components=use_n_pcs).fit_transform(boot_X)\n","    \n","#     # generate DR visualization embedding\n","#     X_embedded = dimensionality_reduction(boot_X, method, **kwargs)\n","\n","\n","#     return(X_embedded, boot_idxs)\n","\n","\n","# def generate(X, method, Y=None, B=0, sigma_noise=None, no_bootstrap=False, random_seed=None, save=False,\n","#              num_jobs=None, use_n_pcs=False, subsample=False, return_times=False, **kwargs):\n","#     '''\n","#     Main method for generating aligned bootstrap visualizations, which are the input elements for dynamic visualization.\n","    \n","#     Arguments:\n","#         X = (n x p) numpy array where rows are observations, columns are features\n","#         method = string, dimensionality reduction method to use; options include:\n","#             \"tsne\", \"mds\", \"lle\", \"mlle\", \"isomap\", \"umap\", \"pca\"\n","#         Y = pandas dataframe with same number of rows as X and columns containing relevant metadata to propagate to output\n","#         B = integer, number of bootstraps to generate; if B==0: generates only for the original X\n","#         sigma_noise = None or float, if float, adds zero-centered Gaussian noise to each bootstrap sample with standard deviation sigma_noise\n","#         no_bootstrap = True or False, whether to bootstrap sample for each iteration or not\n","#         random_seed = None or int, if int, uses that value to generate random sequence (i.e. bootstrap sequence will be the same)\n","#         save = False or str, if str, path to save resulting Pandas dataframe as CSV\n","#         num_jobs = None, -1, or \u003e=1 int; if not None, runs multiprocessing with n_jobs, if n_jobs=-1, then uses all available\n","#         use_n_pcs = False or int, specifying to apply PCA and keep to use_n_pcs components to use for method\n","#         subsample = False or int, specifying whether to subsample INSTEAD OF bootstrapping with integer corresponding to size of subsample to take\n","#         return_times = True or False; if not False, returns a dictionary of run times broken down by components as the second output\n","    \n","#     Returns:\n","#         output = Pandas dataframe with \"x1\", \"x2\", \"bootstrap_number\", \"original_index\" as columns, along with columns of Y\n","#                     2D embedding is (x1, x2)\n","#     '''\n","#     # process results into dataframe\n","#     output = pd.DataFrame() # init df to be merged onto\n","    \n","#     # DR on original dataset\n","#     original_embedding = dimensionality_reduction(X, method, **kwargs) # Ex: can specify random_state in **kwargs to remove stochasticity\n","    \n","#     # reference points is the original dataset\n","#     points0 = np.hstack((original_embedding, np.zeros(original_embedding.shape[0]).reshape(original_embedding.shape[0],1)))# append uniform 3rd dimension\n","\n","#     # add basic info\n","#     output[\"x1\"] = points0[:,0]-np.mean(points0[:,0]) # center reference visualization at (0,0)\n","#     output[\"x2\"] = points0[:,1]-np.mean(points0[:,1])\n","#     output[\"original_index\"] = np.arange(len(points0[:,0]))\n","#     output[\"bootstrap_number\"] = -1\n","\n","#     # add metadata\n","#     if isinstance(Y, pd.DataFrame):\n","#         for col in Y.columns:\n","#             output[col] = Y[col].values\n","    \n","#     # keep track of run times\n","#     rt_dict = {}\n","    \n","#     # bootstrap\n","#     start_time = time.time()\n","#     if B \u003e 0:\n","#         bootstrap_embedding_list, bootstrap_indices_list = bootstrap(X, method, B, sigma_noise, no_bootstrap, \n","#                                                                     random_seed, num_jobs, use_n_pcs, subsample, **kwargs)\n","#     bootstrap_time = time.time() - start_time\n","#     rt_dict[\"bootstrapped_DR\"] = bootstrap_time\n","    \n","#     # add bootstraps\n","#     start_time = time.time()\n","#     for i in range(len(bootstrap_embedding_list)):\n","        \n","#         new_df = pd.DataFrame() # new df to merge onto original df\n","        \n","#         points = bootstrap_embedding_list[i]\n","#         points = np.hstack((points, np.zeros(points.shape[0]).reshape(points.shape[0],1)))# append uniform 3rd dimension\n","#         boot_idxs = bootstrap_indices_list[i]\n","        \n","#         # rigid alignment w/ 3d rotation (Kabsch)\n","#         ref_points = points0[boot_idxs,:]\n","#         points[:,0] = points[:,0]-np.mean(points[:,0])\n","#         points[:,1] = points[:,1]-np.mean(points[:,1])\n","#         r = Rotation.align_vectors(ref_points, points)[0]\n","#         rpoints = r.apply(points)\n","        \n","#         # add basic info\n","#         new_df[\"x1\"] = rpoints[:,0]\n","#         new_df[\"x2\"] = rpoints[:,1]\n","#         new_df[\"original_index\"] = boot_idxs\n","#         new_df[\"bootstrap_number\"] = i\n","        \n","#         # add metadata\n","#         if isinstance(Y, pd.DataFrame):\n","#             for col in Y.columns:\n","#                 new_df[col] = Y[col].values[boot_idxs]\n","        \n","#         # merge to original dataframe\n","#         output = pd.concat([output, new_df], axis=0)\n","    \n","#     align_time = time.time() - start_time\n","#     rt_dict[\"alignment_DR\"] = align_time\n","    \n","#     # save output\n","#     if save is not False:\n","#         output.to_csv(save, index=False)\n","    \n","#     if return_times is False:\n","#         return(output)\n","#     else:\n","#         return(output, rt_dict)\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j_hPnIDu5wcE"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOqgbEChyulsfXFDcKeoU1G","machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}